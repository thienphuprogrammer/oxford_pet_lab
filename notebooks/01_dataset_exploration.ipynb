{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('..')\n",
    "from src.config.config import Config\n",
    "from src.config.model_configs import ModelConfigs\n",
    "from src.data.dataset_loader import OxfordPetDatasetLoader\n",
    "from src.data.preprocessing import DataPreprocessor\n",
    "from src.data.augmentation import DataAugmentor\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPUs available:\", tf.src.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(src.config.RANDOM_SEED)\n",
    "np.random.seed(src.config.RANDOM_SEED)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = src.config.FIGURE_SIZE\n",
    "plt.rcParams['figure.dpi'] = src.config.DPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetExplorer:\n",
    "    \"\"\"Dataset exploration and analysis utilities.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.config = Config()\n",
    "        self.loader = OxfordPetDatasetLoader(self.config)\n",
    "        self.preprocessor = DataPreprocessor(self.config)\n",
    "        self.augmentor = DataAugmentor(self.config)\n",
    "        \n",
    "    def load_and_analyze_dataset(self):\n",
    "        \"\"\"Load dataset and perform initial analysis.\"\"\"\n",
    "        print(\"=== Loading Oxford-IIIT Pet Dataset ===\")\n",
    "        \n",
    "        # Load dataset\n",
    "        self.train_ds, self.val_ds, self.test_ds = self.loader.load_dataset()\n",
    "        self.dataset_info = self.loader.get_dataset_info()\n",
    "        \n",
    "        print(f\"Dataset loaded successfully!\")\n",
    "        print(f\"Number of classes: {self.dataset_info['total_classes']}\")\n",
    "        print(f\"Class names: {self.dataset_info['class_names'][:10]}...\")  # Show first 10\n",
    "        \n",
    "        return self.train_ds, self.val_ds, self.test_ds\n",
    "    \n",
    "    def analyze_class_distribution(self, dataset: tf.data.Dataset, split_name: str):\n",
    "        \"\"\"Analyze class distribution in the dataset.\"\"\"\n",
    "        print(f\"\\n=== Analyzing {split_name} Class Distribution ===\")\n",
    "        \n",
    "        # Count labels\n",
    "        labels = []\n",
    "        for sample in dataset:\n",
    "            labels.append(sample['label'].numpy())\n",
    "        \n",
    "        # Create distribution analysis\n",
    "        label_counts = Counter(labels)\n",
    "        class_names = self.dataset_info['class_names']\n",
    "        \n",
    "        # Create DataFrame for analysis\n",
    "        df = pd.DataFrame([\n",
    "            {'class_id': class_id, 'class_name': class_names[class_id], 'count': count, \n",
    "             'breed_type': 'Cat' if class_names[class_id][0].isupper() else 'Dog'}\n",
    "            for class_id, count in label_counts.items()\n",
    "        ])\n",
    "        df = df.sort_values('count', ascending=False)\n",
    "        \n",
    "        print(f\"Total samples: {len(labels)}\")\n",
    "        print(f\"Classes represented: {len(label_counts)}\")\n",
    "        print(f\"Average samples per class: {np.mean(list(label_counts.values())):.1f}\")\n",
    "        print(f\"Min samples per class: {min(label_counts.values())}\")\n",
    "        print(f\"Max samples per class: {max(label_counts.values())}\")\n",
    "        \n",
    "        # Count cats vs dogs\n",
    "        cat_count = len([name for name in class_names if name[0].isupper()])\n",
    "        dog_count = len(class_names) - cat_count\n",
    "        print(f\"Cat breeds: {cat_count}, Dog breeds: {dog_count}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def visualize_class_distribution(self, train_df: pd.DataFrame, val_df: pd.DataFrame):\n",
    "        \"\"\"Visualize class distribution.\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # 1. Top 15 classes by count (training)\n",
    "        top_classes = train_df.head(15)\n",
    "        axes[0, 0].barh(range(len(top_classes)), top_classes['count'])\n",
    "        axes[0, 0].set_yticks(range(len(top_classes)))\n",
    "        axes[0, 0].set_yticklabels(top_classes['class_name'], fontsize=8)\n",
    "        axes[0, 0].set_xlabel('Number of Samples')\n",
    "        axes[0, 0].set_title('Top 15 Classes by Sample Count (Training)')\n",
    "        axes[0, 0].invert_yaxis()\n",
    "        \n",
    "        # 2. Cat vs Dog distribution\n",
    "        breed_counts = train_df.groupby('breed_type')['count'].sum()\n",
    "        axes[0, 1].pie(breed_counts.values, labels=breed_counts.index, autopct='%1.1f%%')\n",
    "        axes[0, 1].set_title('Cat vs Dog Distribution (Training)')\n",
    "        \n",
    "        # 3. Distribution histogram\n",
    "        axes[1, 0].hist(train_df['count'], bins=20, alpha=0.7, label='Training')\n",
    "        axes[1, 0].hist(val_df['count'], bins=20, alpha=0.7, label='Validation')\n",
    "        axes[1, 0].set_xlabel('Samples per Class')\n",
    "        axes[1, 0].set_ylabel('Number of Classes')\n",
    "        axes[1, 0].set_title('Distribution of Samples per Class')\n",
    "        axes[1, 0].legend()\n",
    "        \n",
    "        # 4. Train vs Val comparison for top classes\n",
    "        merged_df = train_df.merge(val_df, on='class_name', suffixes=('_train', '_val'))\n",
    "        top_10_merged = merged_df.head(10)\n",
    "        \n",
    "        x = np.arange(len(top_10_merged))\n",
    "        width = 0.35\n",
    "        \n",
    "        axes[1, 1].bar(x - width/2, top_10_merged['count_train'], width, label='Train', alpha=0.8)\n",
    "        axes[1, 1].bar(x + width/2, top_10_merged['count_val'], width, label='Validation', alpha=0.8)\n",
    "        axes[1, 1].set_xlabel('Classes')\n",
    "        axes[1, 1].set_ylabel('Number of Samples')\n",
    "        axes[1, 1].set_title('Train vs Validation Split (Top 10 Classes)')\n",
    "        axes[1, 1].set_xticks(x)\n",
    "        axes[1, 1].set_xticklabels(top_10_merged['class_name'], rotation=45, ha='right', fontsize=8)\n",
    "        axes[1, 1].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{src.config.PLOTS_DIR}/class_distribution.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def analyze_image_properties(self, dataset: tf.data.Dataset, split_name: str, num_samples: int = 1000):\n",
    "        \"\"\"Analyze image properties like size, aspect ratio, etc.\"\"\"\n",
    "        print(f\"\\n=== Analyzing {split_name} Image Properties ===\")\n",
    "        \n",
    "        image_properties = {\n",
    "            'widths': [],\n",
    "            'heights': [],\n",
    "            'aspect_ratios': [],\n",
    "            'areas': [],\n",
    "            'channels': []\n",
    "        }\n",
    "        \n",
    "        count = 0\n",
    "        for sample in dataset.take(num_samples):\n",
    "            image = sample['image']\n",
    "            height, width, channels = image.shape\n",
    "            \n",
    "            image_properties['widths'].append(width)\n",
    "            image_properties['heights'].append(height)\n",
    "            image_properties['aspect_ratios'].append(width / height)\n",
    "            image_properties['areas'].append(width * height)\n",
    "            image_properties['channels'].append(channels)\n",
    "            \n",
    "            count += 1\n",
    "            if count >= num_samples:\n",
    "                break\n",
    "        \n",
    "        # Calculate statistics\n",
    "        stats = {}\n",
    "        for prop, values in image_properties.items():\n",
    "            stats[prop] = {\n",
    "                'mean': np.mean(values),\n",
    "                'std': np.std(values),\n",
    "                'min': np.min(values),\n",
    "                'max': np.max(values),\n",
    "                'median': np.median(values)\n",
    "            }\n",
    "        \n",
    "        print(f\"Analyzed {count} images:\")\n",
    "        for prop, stat in stats.items():\n",
    "            if prop != 'channels':  # Skip channels as it's always 3\n",
    "                print(f\"{prop.capitalize()}: mean={stat['mean']:.1f}, std={stat['std']:.1f}, \"\n",
    "                      f\"range=[{stat['min']:.0f}, {stat['max']:.0f}]\")\n",
    "        \n",
    "        return image_properties, stats\n",
    "    \n",
    "    def visualize_image_properties(self, image_props: Dict, split_name: str):\n",
    "        \"\"\"Visualize image properties.\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "        \n",
    "        # 1. Width and Height distribution\n",
    "        axes[0, 0].hist(image_props['widths'], bins=30, alpha=0.7, label='Width')\n",
    "        axes[0, 0].hist(image_props['heights'], bins=30, alpha=0.7, label='Height')\n",
    "        axes[0, 0].set_xlabel('Pixels')\n",
    "        axes[0, 0].set_ylabel('Frequency')\n",
    "        axes[0, 0].set_title(f'Image Dimensions Distribution ({split_name})')\n",
    "        axes[0, 0].legend()\n",
    "        \n",
    "        # 2. Aspect ratio distribution\n",
    "        axes[0, 1].hist(image_props['aspect_ratios'], bins=30, alpha=0.7)\n",
    "        axes[0, 1].set_xlabel('Aspect Ratio (Width/Height)')\n",
    "        axes[0, 1].set_ylabel('Frequency')\n",
    "        axes[0, 1].set_title(f'Aspect Ratio Distribution ({split_name})')\n",
    "        axes[0, 1].axvline(x=1.0, color='red', linestyle='--', label='Square (1:1)')\n",
    "        axes[0, 1].legend()\n",
    "        \n",
    "        # 3. Area distribution\n",
    "        areas_k = [area/1000 for area in image_props['areas']]  # Convert to thousands\n",
    "        axes[1, 0].hist(areas_k, bins=30, alpha=0.7)\n",
    "        axes[1, 0].set_xlabel('Area (thousands of pixels)')\n",
    "        axes[1, 0].set_ylabel('Frequency')\n",
    "        axes[1, 0].set_title(f'Image Area Distribution ({split_name})')\n",
    "        \n",
    "        # 4. Width vs Height scatter\n",
    "        axes[1, 1].scatter(image_props['widths'], image_props['heights'], alpha=0.5)\n",
    "        axes[1, 1].set_xlabel('Width (pixels)')\n",
    "        axes[1, 1].set_ylabel('Height (pixels)')\n",
    "        axes[1, 1].set_title(f'Width vs Height ({split_name})')\n",
    "        # Add diagonal line for square images\n",
    "        max_dim = max(max(image_props['widths']), max(image_props['heights']))\n",
    "        axes[1, 1].plot([0, max_dim], [0, max_dim], 'r--', alpha=0.5, label='Square')\n",
    "        axes[1, 1].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{src.config.PLOTS_DIR}/image_properties_{split_name.lower()}.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def analyze_bounding_boxes(self, dataset: tf.data.Dataset, split_name: str, num_samples: int = 1000):\n",
    "        \"\"\"Analyze bounding box properties.\"\"\"\n",
    "        print(f\"\\n=== Analyzing {split_name} Bounding Box Properties ===\")\n",
    "        \n",
    "        bbox_properties = {\n",
    "            'widths': [],\n",
    "            'heights': [],\n",
    "            'areas': [],\n",
    "            'aspect_ratios': [],\n",
    "            'center_x': [],\n",
    "            'center_y': []\n",
    "        }\n",
    "        \n",
    "        count = 0\n",
    "        for sample in dataset.take(num_samples):\n",
    "            # Process sample to get normalized bbox\n",
    "            processed = self.preprocessor.preprocess_sample(sample)\n",
    "            bbox = processed['bbox'].numpy()  # [xmin, ymin, xmax, ymax] normalized\n",
    "            \n",
    "            xmin, ymin, xmax, ymax = bbox\n",
    "            width = xmax - xmin\n",
    "            height = ymax - ymin\n",
    "            area = width * height\n",
    "            aspect_ratio = width / height if height > 0 else 0\n",
    "            center_x = (xmin + xmax) / 2\n",
    "            center_y = (ymin + ymax) / 2\n",
    "            \n",
    "            bbox_properties['widths'].append(width)\n",
    "            bbox_properties['heights'].append(height)\n",
    "            bbox_properties['areas'].append(area)\n",
    "            bbox_properties['aspect_ratios'].append(aspect_ratio)\n",
    "            bbox_properties['center_x'].append(center_x)\n",
    "            bbox_properties['center_y'].append(center_y)\n",
    "            \n",
    "            count += 1\n",
    "            if count >= num_samples:\n",
    "                break\n",
    "        \n",
    "        # Calculate statistics\n",
    "        stats = {}\n",
    "        for prop, values in bbox_properties.items():\n",
    "            stats[prop] = {\n",
    "                'mean': np.mean(values),\n",
    "                'std': np.std(values),\n",
    "                'min': np.min(values),\n",
    "                'max': np.max(values),\n",
    "                'median': np.median(values)\n",
    "            }\n",
    "        \n",
    "        print(f\"Analyzed {count} bounding boxes:\")\n",
    "        for prop, stat in stats.items():\n",
    "            print(f\"{prop.capitalize()}: mean={stat['mean']:.3f}, std={stat['std']:.3f}, \"\n",
    "                  f\"range=[{stat['min']:.3f}, {stat['max']:.3f}]\")\n",
    "        \n",
    "        return bbox_properties, stats\n",
    "    \n",
    "    def visualize_bounding_boxes(self, bbox_props: Dict, split_name: str):\n",
    "        \"\"\"Visualize bounding box properties.\"\"\"\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "        \n",
    "        # 1. Width and Height distribution\n",
    "        axes[0, 0].hist(bbox_props['widths'], bins=30, alpha=0.7, label='Width')\n",
    "        axes[0, 0].hist(bbox_props['heights'], bins=30, alpha=0.7, label='Height')\n",
    "        axes[0, 0].set_xlabel('Normalized Size')\n",
    "        axes[0, 0].set_ylabel('Frequency')\n",
    "        axes[0, 0].set_title(f'BBox Dimensions ({split_name})')\n",
    "        axes[0, 0].legend()\n",
    "        \n",
    "        # 2. Area distribution\n",
    "        axes[0, 1].hist(bbox_props['areas'], bins=30, alpha=0.7)\n",
    "        axes[0, 1].set_xlabel('Normalized Area')\n",
    "        axes[0, 1].set_ylabel('Frequency')\n",
    "        axes[0, 1].set_title(f'BBox Area Distribution ({split_name})')\n",
    "        \n",
    "        # 3. Aspect ratio distribution\n",
    "        axes[0, 2].hist(bbox_props['aspect_ratios'], bins=30, alpha=0.7)\n",
    "        axes[0, 2].set_xlabel('Aspect Ratio (Width/Height)')\n",
    "        axes[0, 2].set_ylabel('Frequency')\n",
    "        axes[0, 2].set_title(f'BBox Aspect Ratio ({split_name})')\n",
    "        axes[0, 2].axvline(x=1.0, color='red', linestyle='--', label='Square')\n",
    "        axes[0, 2].legend()\n",
    "        \n",
    "        # 4. Center position heatmap\n",
    "        axes[1, 0].hist2d(bbox_props['center_x'], bbox_props['center_y'], bins=20, cmap='Blues')\n",
    "        axes[1, 0].set_xlabel('Center X (normalized)')\n",
    "        axes[1, 0].set_ylabel('Center Y (normalized)')\n",
    "        axes[1, 0].set_title(f'BBox Center Distribution ({split_name})')\n",
    "        axes[1, 0].set_xlim(0, 1)\n",
    "        axes[1, 0].set_ylim(0, 1)\n",
    "        \n",
    "        # 5. Width vs Height scatter\n",
    "        axes[1, 1].scatter(bbox_props['widths'], bbox_props['heights'], alpha=0.5)\n",
    "        axes[1, 1].set_xlabel('Width (normalized)')\n",
    "        axes[1, 1].set_ylabel('Height (normalized)')\n",
    "        axes[1, 1].set_title(f'BBox Width vs Height ({split_name})')\n",
    "        # Add diagonal line for square bboxes\n",
    "        max_dim = max(max(bbox_props['widths']), max(bbox_props['heights']))\n",
    "        axes[1, 1].plot([0, max_dim], [0, max_dim], 'r--', alpha=0.5, label='Square')\n",
    "        axes[1, 1].legend()\n",
    "        \n",
    "        # 6. Area vs Aspect Ratio\n",
    "        axes[1, 2].scatter(bbox_props['areas'], bbox_props['aspect_ratios'], alpha=0.5)\n",
    "        axes[1, 2].set_xlabel('Area (normalized)')\n",
    "        axes[1, 2].set_ylabel('Aspect Ratio')\n",
    "        axes[1, 2].set_title(f'BBox Area vs Aspect Ratio ({split_name})')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{src.config.PLOTS_DIR}/bbox_properties_{split_name.lower()}.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def visualize_sample_data(self, dataset: tf.data.Dataset, num_samples: int = 8):\n",
    "        \"\"\"Visualize sample images with annotations.\"\"\"\n",
    "        print(f\"\\n=== Visualizing Sample Data ===\")\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        class_names = self.dataset_info['class_names']\n",
    "        \n",
    "        count = 0\n",
    "        for sample in dataset.take(num_samples):\n",
    "            # Preprocess sample\n",
    "            processed = self.preprocessor.preprocess_sample(sample)\n",
    "            \n",
    "            image = processed['image'].numpy()\n",
    "            label = processed['label'].numpy()\n",
    "            bbox = processed['bbox'].numpy()  # normalized\n",
    "            seg_mask = processed['segmentation_mask'].numpy()\n",
    "            \n",
    "            # Create visualization\n",
    "            ax = axes[count]\n",
    "            \n",
    "            # Show image\n",
    "            ax.imshow(image)\n",
    "            \n",
    "            # Draw bounding box\n",
    "            xmin, ymin, xmax, ymax = bbox\n",
    "            # Convert normalized coordinates to pixel coordinates\n",
    "            h, w = image.shape[:2]\n",
    "            xmin_px, ymin_px = xmin * w, ymin * h\n",
    "            xmax_px, ymax_px = xmax * w, ymax * h\n",
    "            \n",
    "            from matplotlib.patches import Rectangle\n",
    "            rect = Rectangle((xmin_px, ymin_px), xmax_px - xmin_px, ymax_px - ymin_px,\n",
    "                           linewidth=2, edgecolor='red', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            # Set title with class name\n",
    "            class_name = class_names[label]\n",
    "            breed_type = \"Cat\" if class_name[0].isupper() else \"Dog\"\n",
    "            ax.set_title(f'{class_name} ({breed_type})', fontsize=10)\n",
    "            ax.axis('off')\n",
    "            \n",
    "            count += 1\n",
    "            if count >= num_samples:\n",
    "                break\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{src.config.PLOTS_DIR}/sample_src.data.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def visualize_segmentation_masks(self, dataset: tf.data.Dataset, num_samples: int = 6):\n",
    "        \"\"\"Visualize segmentation masks.\"\"\"\n",
    "        print(f\"\\n=== Visualizing Segmentation Masks ===\")\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 6, figsize=(18, 6))\n",
    "        \n",
    "        class_names = self.dataset_info['class_names']\n",
    "        \n",
    "        count = 0\n",
    "        for sample in dataset.take(num_samples):\n",
    "            # Preprocess sample\n",
    "            processed = self.preprocessor.preprocess_sample(sample)\n",
    "            \n",
    "            image = processed['image'].numpy()\n",
    "            label = processed['label'].numpy()\n",
    "            seg_mask = processed['segmentation_mask'].numpy()\n",
    "            \n",
    "            # Original image\n",
    "            axes[0, count].imshow(image)\n",
    "            axes[0, count].set_title(f'{class_names[label]}', fontsize=10)\n",
    "            axes[0, count].axis('off')\n",
    "            \n",
    "            # Segmentation mask\n",
    "            # Create color map: 0=background (blue), 1=foreground (red), 2=unknown (green)\n",
    "            mask_colored = np.zeros((*seg_mask.shape, 3))\n",
    "            mask_colored[seg_mask == 0] = [0, 0, 1]    # Background - blue\n",
    "            mask_colored[seg_mask == 1] = [1, 0, 0]    # Foreground - red  \n",
    "            mask_colored[seg_mask == 2] = [0, 1, 0]    # Unknown - green\n",
    "            \n",
    "            axes[1, count].imshow(mask_colored)\n",
    "            axes[1, count].set_title('Segmentation Mask', fontsize=10)\n",
    "            axes[1, count].axis('off')\n",
    "            \n",
    "            count += 1\n",
    "            if count >= num_samples:\n",
    "                break\n",
    "        \n",
    "        # Add legend\n",
    "        from matplotlib.patches import Patch\n",
    "        legend_elements = [\n",
    "            Patch(facecolor='blue', label='Background'),\n",
    "            Patch(facecolor='red', label='Foreground (Pet)'),\n",
    "            Patch(facecolor='green', label='Unknown')\n",
    "        ]\n",
    "        fig.legend(handles=legend_elements, loc='upper center', bbox_to_anchor=(0.5, 0.95), ncol=3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{src.config.PLOTS_DIR}/segmentation_masks.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def test_preprocessing_pipeline(self):\n",
    "        \"\"\"Test the preprocessing pipeline.\"\"\"\n",
    "        print(f\"\\n=== Testing Preprocessing Pipeline ===\")\n",
    "        \n",
    "        # Test with a single sample\n",
    "        sample = next(iter(self.train_ds.take(1)))\n",
    "        print(\"Original sample keys:\", list(sample.keys()))\n",
    "        print(\"Original image shape:\", sample['image'].shape)\n",
    "        print(\"Original bbox:\", sample['bbox'].numpy())\n",
    "        print(\"Original label:\", sample['label'].numpy())\n",
    "        print(\"Original seg mask shape:\", sample['segmentation_mask'].shape)\n",
    "        \n",
    "        # Process sample\n",
    "        processed = self.preprocessor.preprocess_sample(sample)\n",
    "        print(\"\\nProcessed sample keys:\", list(processed.keys()))\n",
    "        print(\"Processed image shape:\", processed['image'].shape)\n",
    "        print(\"Processed bbox (normalized):\", processed['bbox'].numpy())\n",
    "        print(\"Processed label:\", processed['label'].numpy())\n",
    "        print(\"Processed seg mask shape:\", processed['segmentation_mask'].shape)\n",
    "        print(\"Processed seg mask unique values:\", np.unique(processed['segmentation_mask'].numpy()))\n",
    "        \n",
    "        # Test augmentation\n",
    "        image = processed['image']\n",
    "        bbox = processed['bbox'] \n",
    "        mask = processed['segmentation_mask']\n",
    "        \n",
    "        aug_image, aug_bbox, aug_mask = self.augmentor.augment_sample(image, bbox, mask)\n",
    "        print(\"\\nAugmented image shape:\", aug_image.shape)\n",
    "        print(\"Augmented bbox:\", aug_bbox.numpy())\n",
    "        print(\"Augmented mask shape:\", aug_mask.shape)\n",
    "        \n",
    "        print(\"Preprocessing pipeline test completed successfully!\")\n",
    "    \n",
    "    def create_processed_datasets(self):\n",
    "        \"\"\"Create preprocessed datasets for training.\"\"\"\n",
    "        print(f\"\\n=== Creating Processed Datasets ===\")\n",
    "        \n",
    "        def preprocess_fn(sample):\n",
    "            return self.preprocessor.preprocess_sample(sample)\n",
    "        \n",
    "        def augment_fn(sample):\n",
    "            processed = preprocess_fn(sample)\n",
    "            image, bbox, mask = self.augmentor.augment_sample(\n",
    "                processed['image'], processed['bbox'], processed['segmentation_mask']\n",
    "            )\n",
    "            processed['image'] = image\n",
    "            processed['bbox'] = bbox  \n",
    "            processed['segmentation_mask'] = mask\n",
    "            return processed\n",
    "        \n",
    "        # Create preprocessed datasets\n",
    "        train_processed = self.train_ds.map(augment_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        val_processed = self.val_ds.map(preprocess_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        test_processed = self.test_ds.map(preprocess_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        \n",
    "        # Configure for performance\n",
    "        train_processed = train_processed.batch(src.config.BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "        val_processed = val_processed.batch(src.config.BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "        test_processed = test_processed.batch(src.config.BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        print(\"Processed datasets created successfully!\")\n",
    "        return train_processed, val_processed, test_processed\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize explorer\n",
    "    explorer = DatasetExplorer()\n",
    "    \n",
    "    # Load and analyze dataset\n",
    "    train_ds, val_ds, test_ds = explorer.load_and_analyze_dataset()\n",
    "    \n",
    "    # Analyze class distributions\n",
    "    train_class_df = explorer.analyze_class_distribution(train_ds, \"Training\")\n",
    "    val_class_df = explorer.analyze_class_distribution(val_ds, \"Validation\")\n",
    "    \n",
    "    # Visualize class distributions\n",
    "    explorer.visualize_class_distribution(train_class_df, val_class_df)\n",
    "    \n",
    "    # Analyze image properties\n",
    "    train_img_props, train_img_stats = explorer.analyze_image_properties(train_ds, \"Training\")\n",
    "    explorer.visualize_image_properties(train_img_props, \"Training\")\n",
    "    \n",
    "    # Analyze bounding boxes\n",
    "    train_bbox_props, train_bbox_stats = explorer.analyze_bounding_boxes(train_ds, \"Training\")\n",
    "    explorer.visualize_bounding_boxes(train_bbox_props, \"Training\")\n",
    "    \n",
    "    # Visualize sample data\n",
    "    explorer.visualize_sample_data(train_ds)\n",
    "    explorer.visualize_segmentation_masks(train_ds)\n",
    "    \n",
    "    # Test preprocessing pipeline\n",
    "    explorer.test_preprocessing_pipeline()\n",
    "    \n",
    "    # Create processed datasets for next tasks\n",
    "    train_processed, val_processed, test_processed = explorer.create_processed_datasets()\n",
    "    \n",
    "    print(\"\\n=== Task 1 Completed Successfully! ===\")\n",
    "    print(\"Next steps:\")\n",
    "    print(\"1. Run Task 2: Object Detection\")\n",
    "    print(\"2. Run Task 3: Semantic Segmentation\") \n",
    "    print(\"3. Run Task 4: Multitask Learning\")\n",
    "    print(\"4. Generate comprehensive report\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
