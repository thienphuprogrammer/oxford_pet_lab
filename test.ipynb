{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07f964a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 02:10:57.197215: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-17 02:10:57.205328: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750101057.214777   35508 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750101057.217418   35508 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750101057.224638   35508 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750101057.224648   35508 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750101057.224649   35508 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750101057.224650   35508 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-17 02:10:57.227798: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "I0000 00:00:1750101058.572317   35508 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6115 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "from src.data import OxfordPetDatasetLoader\n",
    "from src.config import Config\n",
    "from src.data import DataPreprocessor, DataAugmentor\n",
    "\n",
    "\n",
    "config = Config()\n",
    "config.USE_IMAGENET_NORM = True          # For transfer learning\n",
    "config.PRESERVE_ASPECT_RATIO = True      # Better image quality\n",
    "config.ENABLE_QUALITY_ENHANCEMENT = True # Image enhancement\n",
    "config.NORMALIZATION_METHOD = 'imagenet' # Best for pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae3942e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data.dataset_loader:Initialized Oxford Pet Dataset Loader\n",
      "INFO:src.data.dataset_loader:Data directory: ./data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loader = OxfordPetDatasetLoader(\n",
    "    data_dir=\"./data\",  # Tùy chọn\n",
    "    download=True,\n",
    "    log_level='INFO'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9812d2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data.dataset_loader:Creating train/validation/test splits...\n",
      "INFO:src.data.dataset_loader:Checking dataset availability...\n",
      "INFO:absl:Load dataset info from data/oxford_iiit_pet/4.0.0\n",
      "INFO:src.data.dataset_loader:Dataset already available.\n",
      "INFO:src.data.dataset_loader:Loading raw dataset splits: ['train', 'test']\n",
      "INFO:absl:Load dataset info from data/oxford_iiit_pet/4.0.0\n",
      "INFO:absl:Creating a tf.data.Dataset reading 4 files located in folders: data/oxford_iiit_pet/4.0.0.\n",
      "INFO:absl:Creating a tf.data.Dataset reading 4 files located in folders: data/oxford_iiit_pet/4.0.0.\n",
      "INFO:absl:Constructing tf.data.Dataset oxford_iiit_pet for split ['train', 'test'], from data/oxford_iiit_pet/4.0.0\n",
      "INFO:src.data.dataset_loader:Train set: 3,680 examples\n",
      "INFO:src.data.dataset_loader:Test set: 3,669 examples\n",
      "INFO:src.data.dataset_loader:Split sizes - Train: 2,944, Val: 736, Test: 3,669\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_ds, val_ds, test_ds = loader.create_train_val_test_splits(\n",
    "    val_split=0.2,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a53329a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_name': TensorSpec(shape=(), dtype=tf.string, name=None),\n",
       " 'head_bbox': TensorSpec(shape=(4,), dtype=tf.float32, name=None),\n",
       " 'image': TensorSpec(shape=(None, None, 3), dtype=tf.uint8, name=None),\n",
       " 'label': TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
       " 'segmentation_mask': TensorSpec(shape=(None, None, 1), dtype=tf.uint8, name=None),\n",
       " 'species': TensorSpec(shape=(), dtype=tf.int64, name=None)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdba002f",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentor = DataAugmentor(\n",
    "    config=config,\n",
    "    prob_geo=0.7,\n",
    "    prob_photo=0.8,\n",
    "    prob_mixup=0.3,\n",
    "    prob_cutout=0.2,\n",
    "    prob_mosaic=0.3,\n",
    ")\n",
    "\n",
    "train_ds = augmentor.create_augmented_dataset(\n",
    "    train_ds,\n",
    "    augmentation_factor=3\n",
    ")\n",
    "\n",
    "val_ds = augmentor.create_augmented_dataset(\n",
    "    val_ds,\n",
    "    augmentation_factor=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91b41f49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': TensorSpec(shape=(224, 224, 3), dtype=tf.float32, name=None),\n",
       " 'head_bbox': TensorSpec(shape=(4,), dtype=tf.float32, name=None),\n",
       " 'segmentation_mask': TensorSpec(shape=(224, 224, 1), dtype=tf.float32, name=None),\n",
       " 'label': TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
       " 'species': TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
       " 'file_name': TensorSpec(shape=(), dtype=tf.string, name=None)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56514ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = DataPreprocessor(\n",
    "    config=Config(),\n",
    "    shuffle_buffer=5000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91debd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = processor.create_training_dataset(\n",
    "    train_ds,\n",
    "    batch_size=8,\n",
    "    task=\"detection\",\n",
    "    cache_filename=\"train_cache\"  # File caching for speed\n",
    ")\n",
    "\n",
    "val_ds = processor.create_validation_dataset(\n",
    "    val_ds,\n",
    "    batch_size=8,\n",
    "    task=\"detection\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd7b0875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(8, 224, 224, 3), dtype=tf.float32, name=None),\n",
       " {'bbox': TensorSpec(shape=(8, 4), dtype=tf.float32, name=None),\n",
       "  'pet_class': TensorSpec(shape=(8,), dtype=tf.int32, name=None),\n",
       "  'species': TensorSpec(shape=(8,), dtype=tf.int32, name=None),\n",
       "  'valid_bbox': TensorSpec(shape=(8,), dtype=tf.bool, name=None),\n",
       "  'area': TensorSpec(shape=(8,), dtype=tf.float32, name=None)})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "270c5b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.base_model import ModelBuilder\n",
    "from src.config.config import Config\n",
    "\n",
    "\n",
    "model = ModelBuilder.build_detection_model(\n",
    "    model_type=\"pretrained\",\n",
    "    num_classes=37,\n",
    "    config=Config(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cc60a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 02:11:05,048 - trainer_135924244493072 - WARNING - Memory growth setting failed: Physical devices cannot be modified after being initialized\n"
     ]
    }
   ],
   "source": [
    "from src.training.trainer import Trainer\n",
    "from src.config.config import Config\n",
    "from src.config.model_configs import ModelConfigs\n",
    "from pathlib import Path\n",
    "\n",
    "config = Config()\n",
    "config.CHECKPOINT_DIR = Path(\"./checkpoints\")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    task_type=\"detection\",\n",
    "    backbone_name=\"resnet50\",\n",
    "    config=config,\n",
    "    models_config=ModelConfigs(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac917ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 02:11:05.901742: I tensorflow/core/kernels/data/tf_record_dataset_op.cc:387] The default buffer size is 262144, which is overridden by the user specified `buffer_size` of 8388608\n",
      "2025-06-17 02:11:15.360725: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] fused(ShuffleDatasetV3:20,RepeatDataset:21): Filling up shuffle buffer (this may take a while): 1174 of 5000\n",
      "2025-06-17 02:11:25.366062: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] fused(ShuffleDatasetV3:20,RepeatDataset:21): Filling up shuffle buffer (this may take a while): 2470 of 5000\n",
      "2025-06-17 02:11:43.908579: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:482] Shuffle buffer filled.\n",
      "I0000 00:00:1750101107.127187   35602 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2025-06-17 02:11:50,037 - trainer_135924244493072 - ERROR - Error in training step 0: No gradients computed. This usually means:\n",
      "1. The loss is not connected to model variables\n",
      "2. The model is not being called properly in train_step()\n",
      "3. Variables are not marked as trainable\n",
      "Model has 228 trainable variables\n",
      "2025-06-17 02:11:50.039095: W tensorflow/core/kernels/data/cache_dataset_ops.cc:333] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2025-06-17 02:11:50,089 - trainer_135924244493072 - ERROR - Training failed with error: No gradients computed. This usually means:\n",
      "1. The loss is not connected to model variables\n",
      "2. The model is not being called properly in train_step()\n",
      "3. Variables are not marked as trainable\n",
      "Model has 228 trainable variables\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/alexander/Workspace/Project/oxford_pet_lab/src/training/base.py\", line 508, in fit\n",
      "    self._run_training_loop(train_dataset, val_dataset, epochs, validation_freq)\n",
      "  File \"/home/alexander/Workspace/Project/oxford_pet_lab/src/training/base.py\", line 526, in _run_training_loop\n",
      "    epoch_metrics = self.train_epoch(train_dataset, val_dataset if epoch % validation_freq == 0 else None)\n",
      "  File \"/home/alexander/Workspace/Project/oxford_pet_lab/src/training/base.py\", line 290, in train_epoch\n",
      "    train_metrics = self._run_training_phase(train_dataset)\n",
      "  File \"/home/alexander/Workspace/Project/oxford_pet_lab/src/training/base.py\", line 335, in _run_training_phase\n",
      "    step_metrics = self._training_step_with_accumulation(batch, batch_idx)\n",
      "  File \"/home/alexander/Workspace/Project/oxford_pet_lab/src/training/base.py\", line 395, in _training_step_with_accumulation\n",
      "    raise ValueError(\n",
      "ValueError: No gradients computed. This usually means:\n",
      "1. The loss is not connected to model variables\n",
      "2. The model is not being called properly in train_step()\n",
      "3. Variables are not marked as trainable\n",
      "Model has 228 trainable variables\n",
      "2025-06-17 02:11:50,093 - trainer_135924244493072 - INFO - Training completed in 45.03 seconds\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No gradients computed. This usually means:\n1. The loss is not connected to model variables\n2. The model is not being called properly in train_step()\n3. Variables are not marked as trainable\nModel has 228 trainable variables",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Workspace/Project/oxford_pet_lab/src/training/base.py:508\u001b[0m, in \u001b[0;36mBaseTrainer.fit\u001b[0;34m(self, train_dataset, val_dataset, epochs, resume_from_checkpoint, validation_freq)\u001b[0m\n\u001b[1;32m    505\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 508\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_training_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining interrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Workspace/Project/oxford_pet_lab/src/training/base.py:526\u001b[0m, in \u001b[0;36mBaseTrainer._run_training_loop\u001b[0;34m(self, train_dataset, val_dataset, epochs, validation_freq)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mcurrent_epoch \u001b[38;5;241m=\u001b[39m epoch\n\u001b[1;32m    525\u001b[0m \u001b[38;5;66;03m# Training epoch\u001b[39;00m\n\u001b[0;32m--> 526\u001b[0m epoch_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;66;03m# Execute callbacks\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_callbacks(epoch, epoch_metrics)\n",
      "File \u001b[0;32m~/Workspace/Project/oxford_pet_lab/src/training/base.py:290\u001b[0m, in \u001b[0;36mBaseTrainer.train_epoch\u001b[0;34m(self, train_dataset, val_dataset)\u001b[0m\n\u001b[1;32m    287\u001b[0m     metric\u001b[38;5;241m.\u001b[39mreset_state()\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# Training phase\u001b[39;00m\n\u001b[0;32m--> 290\u001b[0m train_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_training_phase\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;66;03m# Validation phase\u001b[39;00m\n\u001b[1;32m    293\u001b[0m val_metrics \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/Workspace/Project/oxford_pet_lab/src/training/base.py:335\u001b[0m, in \u001b[0;36mBaseTrainer._run_training_phase\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    331\u001b[0m step_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;66;03m# Training step with gradient accumulation\u001b[39;00m\n\u001b[0;32m--> 335\u001b[0m     step_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_training_step_with_accumulation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m step_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    338\u001b[0m     num_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Workspace/Project/oxford_pet_lab/src/training/base.py:395\u001b[0m, in \u001b[0;36mBaseTrainer._training_step_with_accumulation\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;66;03m# Check if gradients are None (indicates no connection between loss and variables)\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gradients \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mall\u001b[39m(g \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m gradients):\n\u001b[0;32m--> 395\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    396\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo gradients computed. This usually means:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    397\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1. The loss is not connected to model variables\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    398\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2. The model is not being called properly in train_step()\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3. Variables are not marked as trainable\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    400\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrainable_variables)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m trainable variables\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    401\u001b[0m     )\n\u001b[1;32m    403\u001b[0m \u001b[38;5;66;03m# Filter out None gradients and corresponding variables\u001b[39;00m\n\u001b[1;32m    404\u001b[0m valid_grads_and_vars \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    405\u001b[0m     (grad, var) \u001b[38;5;28;01mfor\u001b[39;00m grad, var \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(gradients, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrainable_variables) \n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    407\u001b[0m ]\n",
      "\u001b[0;31mValueError\u001b[0m: No gradients computed. This usually means:\n1. The loss is not connected to model variables\n2. The model is not being called properly in train_step()\n3. Variables are not marked as trainable\nModel has 228 trainable variables"
     ]
    }
   ],
   "source": [
    "trainer.fit(train_ds, val_dataset=val_ds, epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
